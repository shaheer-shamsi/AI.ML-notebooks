{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6180fd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aae4a8",
   "metadata": {},
   "source": [
    "# Understanding Types of Deep Learning Algorithms\n",
    "\n",
    "In this section, we will explore the different types of deep learning algorithms and their key characteristics, including supervised learning, unsupervised learning, hybrid learning, and reinforcement learning.\n",
    "\n",
    "## Supervised Learning\n",
    "\n",
    "Supervised learning involves training algorithms with labeled data, where inputs are associated with corresponding output labels. The aim is to enable the algorithm to predict accurate results on new, unseen data. Tasks like classification and regression fall under this category, where the algorithm learns to classify inputs into categories or predict continuous values.\n",
    "\n",
    "### Types of Supervised Learning\n",
    "\n",
    "Supervised learning is a type of machine learning where the algorithm is trained on a labeled dataset. It can be divided into two primary types of tasks:\n",
    "\n",
    "#### Classification:\n",
    "\n",
    "**Description:** Classification is a type of supervised learning where the goal is to predict the categorical class label of a new instance, based on past observations. In classification, the output variable is a category, such as \"spam\" or \"not spam,\" \"fraudulent\" or \"legitimate,\" or \"cat,\" \"dog,\" or \"horse.\"\n",
    "\n",
    "**Evaluation Parameter:** The primary evaluation parameters for classification tasks are:\n",
    "\n",
    "- **Accuracy**: Accuracy measures the proportion of correctly classified instances out of the total instances. It is a commonly used metric for classification problems. However, it may not be the best choice when dealing with imbalanced datasets.\n",
    "\n",
    "Other evaluation parameters for classification include precision, recall, F1-score, ROC-AUC, and confusion matrix.\n",
    "\n",
    "#### Regression:\n",
    "\n",
    "**Description:** Regression is another type of supervised learning where the goal is to predict a continuous numeric output variable. In regression, the output variable is a real or continuous value, such as predicting house prices, stock prices, or temperature.\n",
    "\n",
    "**Evaluation Parameter:** The primary evaluation parameters for regression tasks are:\n",
    "\n",
    "- **Mean Squared Error (MSE)**: MSE measures the average squared difference between the predicted values and the actual target values. It penalizes large errors more heavily, making it sensitive to outliers.\n",
    "\n",
    "- **Root Mean Squared Error (RMSE)**: RMSE is the square root of the MSE and is often used because it provides an error measure in the same units as the target variable, making it more interpretable.\n",
    "\n",
    "Other evaluation parameters for regression include Mean Absolute Error (MAE), R-squared (R²), and residual plots.\n",
    "\n",
    "In summary, supervised learning can be divided into two main categories: classification and regression. For classification tasks, accuracy is a common evaluation parameter, while for regression tasks, MSE and RMSE are commonly used evaluation parameters. The choice of evaluation parameter depends on the specific problem and the nature of the output variable.\n",
    "\n",
    "\n",
    "## Unsupervised Learning\n",
    "\n",
    "Unsupervised learning trains algorithms on data without explicit output labels. The goal is to uncover underlying patterns or structures within the data. Tasks like clustering and dimensionality reduction are common in this category. Clustering groups similar data points together, while dimensionality reduction reduces data complexity while retaining essential features.\n",
    "\n",
    "## Semi-Supervised Learning\n",
    "\n",
    "Semi-Supervised learning combines aspects of both supervised and unsupervised learning. It utilizes labeled data for initial training and then employs unsupervised techniques to refine the model. Semi-supervised learning, for example, trains a model on a small amount of labeled data and a larger amount of unlabeled data. This approach improves model performance while reducing the need for extensive labeled data.\n",
    "\n",
    "## Reinforcement Learning\n",
    "\n",
    "Reinforcement learning is a different paradigm where an agent learns by interacting with an environment. The agent takes actions to maximize a reward signal over time. It learns through exploration and exploitation, adjusting its actions based on feedback from the environment. Reinforcement learning is commonly used in scenarios like game playing and robotics.\n",
    "\n",
    "These different types of deep learning algorithms serve distinct purposes and contribute to the versatility and effectiveness of modern machine learning and artificial intelligence applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3886e2e3",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "Classification is a fundamental task in machine learning that involves categorizing data into different classes or categories based on certain features or characteristics. It aims to find patterns or relationships in the data that can help assign new instances to the appropriate class. \n",
    "\n",
    "## Binary Classification\n",
    "\n",
    "Binary classification is a type of classification problem where the data is divided into two distinct classes or categories. It involves predicting whether an instance belongs to one class or another. Typically, the classes are represented as \"positive\" and \"negative,\" \"yes\" and \"no,\" or \"1\" and \"0.\" \n",
    "\n",
    "### Binary Classifiers and Techniques\n",
    "\n",
    "Numerous machine learning applications use binary classifiers as a fundamental building component. A variety of techniques can be used to create them, including:\n",
    "\n",
    "- Logistic regression\n",
    "- Support vector machines (SVMs)\n",
    "- Decision trees\n",
    "- Random forests\n",
    "- Neural networks\n",
    "\n",
    "Typically, these models are trained on labeled data, where the right label or category for each example in the training set is known. They are then employed in predicting the category of new, unseen samples.\n",
    "\n",
    "### Metrics frequently used to evaluate a binary classifier's performance include:\n",
    "\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 score\n",
    "\n",
    "These metrics measure how effectively the model is able to correctly identify positive and negative examples in the data. A wide range of applications, such as natural language processing, computer vision, fraud detection, and medical diagnostics, among many others, require high-quality binary classifiers.\n",
    "\n",
    "\n",
    "## Multi-Class Classification\n",
    "\n",
    "A classification problem including more than two classes, such as classifying a collection of fruit photos that could represent oranges, apples, or pears. A fruit may only be either an apple or a pear, not both at once, according to the multi-class classification method, which operates under the assumption that each sample is given one and only one label.\n",
    "\n",
    "<img src=\"Lecture_Image_Files/lecture_15/fruit.png\" width=\"300\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac1e2d7",
   "metadata": {},
   "source": [
    "# Loss Function vs. Cost Function\n",
    "\n",
    "## Loss Function:\n",
    "\n",
    "A **loss function**, sometimes referred to as the error function or objective function, is a mathematical function used to measure the error or discrepancy between the predicted output (hypothesis) of a machine learning model and the actual target values (ground truth) for a single data point. The primary purpose of the loss function is to quantify how well or poorly the model is performing on a single data instance.\n",
    "\n",
    "**Key Points:**\n",
    "- Loss functions are used during the training phase of machine learning models.\n",
    "- They assess the error for a single data point or a mini-batch of data.\n",
    "- Common loss functions include Mean Squared Error (MSE) for regression tasks and Cross-Entropy for classification tasks.\n",
    "- The goal is to minimize the loss, making the model's predictions more accurate for individual data points.\n",
    "\n",
    "## Cost Function:\n",
    "\n",
    "A **cost function**, also known as the objective function or simply the cost, is a function that aggregates or summarizes the loss values computed for individual data points across the entire dataset. It provides a single scalar value that represents the overall performance of the model on the entire dataset. The cost function serves as a global measure of how well the model is doing.\n",
    "\n",
    "**Key Points:**\n",
    "- Cost functions are used to evaluate the performance of a machine learning model after a complete pass through the training dataset (epoch).\n",
    "- They calculate the cumulative error by summing or averaging the individual loss values.\n",
    "- The goal is to minimize the cost function during training, which means making the model's predictions as accurate as possible across the entire dataset.\n",
    "- The choice of the cost function depends on the problem type (e.g., regression, classification) and the specific objectives.\n",
    "\n",
    "**Relationship:**\n",
    "- The loss function and cost function are closely related. The loss function is used to calculate errors for individual data points or mini-batches, while the cost function aggregates these errors to assess the overall model performance. In many cases, the cost function is derived from the average of the loss function over the entire dataset.\n",
    "\n",
    "In summary, the loss function measures the error for a single data point, guiding the model's parameter updates during training, while the cost function provides a global measure of the model's performance by summarizing the loss values across the entire dataset. Both are crucial for training and evaluating machine learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ffa5a1",
   "metadata": {},
   "source": [
    "### Difference Between Loss and Cost Functions\n",
    "\n",
    "| Aspect                  | Loss Function                               | Cost Function                               |\n",
    "|-------------------------|---------------------------------------------|---------------------------------------------|\n",
    "| Purpose                 | Used during training to update model parameters | Aggregates loss values across the entire dataset |\n",
    "| Definition              | Measures the error for a single data point  | Aggregates the losses over all data points   |\n",
    "| Calculation             | Computed for each data point individually   | Sum or average of individual loss values     |\n",
    "| Objective               | Minimize the loss for each individual data point | Minimize the overall cost over the dataset  |\n",
    "| Typically in Neural Networks | Used within each training iteration (mini-batch) | Used to evaluate and optimize the entire model |\n",
    "| Examples                | Mean Squared Error (MSE) for regression     | Cross-Entropy for classification            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62844446",
   "metadata": {},
   "source": [
    "# Mean Squared Error (MSE)\n",
    "\n",
    "\n",
    "In the case of regression tasks, a common loss function is Mean Squared Error (MSE). The mathematical equation for this is:\n",
    "\n",
    "$$ L(θ) = \\frac{1}{N} \\sum_{i=1}^{N} (ŷ_i - y_i)^2 $$\n",
    "\n",
    "In this equation:\n",
    "\n",
    "- $n$ is the total number of observations.\n",
    "- $y_i$ is the actual value of the output for the i-th observation.\n",
    "- $\\hat{y}_i$ is the predicted value of the output for the i-th observation.\n",
    "- $\\sum$ denotes the summation operation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3404fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the actual values and predicted values as NumPy arrays\n",
    "actual_values = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "predicted_values = np.array([1.2, 1.8, 2.7, 3.8, 4.9])\n",
    "\n",
    "# Calculate the squared differences between actual and predicted values\n",
    "squared_errors = (actual_values - predicted_values) ** 2\n",
    "\n",
    "# Calculate the mean of squared errors to get MSE\n",
    "mse = np.mean(squared_errors)\n",
    "\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbd007f",
   "metadata": {},
   "source": [
    "## Regression Analysis in Machine Learning\n",
    "\n",
    "Regression analysis in machine learning involves building predictive models that estimate continuous numerical outcomes from input data. This technique plays a fundamental role in understanding and quantifying relationships between variables. Here's a concise overview with a bit more detail on types of regression:\n",
    "\n",
    "### Objective\n",
    "Regression analysis aims to establish a mathematical relationship between input features and a continuous target variable. This relationship is encapsulated within a model, enabling predictions on unseen data.\n",
    "\n",
    "### Types of Regression\n",
    "There are various regression techniques, each suited to different data scenarios:\n",
    "\n",
    "- **Linear Regression:** Assumes a linear relationship between inputs and the target. It seeks the best-fitting linear equation to predict the target.\n",
    "- **Polynomial Regression:** Models nonlinear relationships by incorporating polynomial terms into the equation.\n",
    "- **Ridge and Lasso Regression:** These techniques extend linear regression, adding regularization to prevent overfitting and handle multicollinearity.\n",
    "- **Decision Tree Regression:** Utilizes decision trees to partition data and make predictions based on leaf node averages.\n",
    "- **Random Forest Regression:** An ensemble method that combines multiple decision trees for improved prediction accuracy.\n",
    "- **Support Vector Regression (SVR):** An adaptation of support vector machines for regression tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9319c05f",
   "metadata": {},
   "source": [
    "### In today's class we will discuss about Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa73efe",
   "metadata": {},
   "source": [
    "## Linear Regression:\n",
    "\n",
    "A supervised machine learning approach known as \"linear regression\" determines the linear relationship between a dependent variable and one or more independent features. Univariate linear regression occurs when there is only one independent feature; multivariate linear regression occurs when there are multiple independent features. The algorithm's objective is to identify the optimum linear equation that, given the independent variables, can forecast the value of the dependent variable. The relationship between the dependent and independent variables is shown by the equation as a straight line. The slope of the line shows how much the dependent variable changes when the independent variable(s) are changed by a unit.\n",
    "    \n",
    "Finance, economics, and psychology are just a few of the fields that employ linear regression to analyze and forecast the behavior of a given variable. For instance, linear regression can be used in the field of finance to comprehend the connection between a company's stock price and its earnings or to forecast the value of a currency based on its past behavior.\n",
    "    \n",
    "Regression is one of the most significant supervised learning tasks. In regression, a series of records with X and Y values are present, and these values are used to train a function that may be used to predict Y from an unknown X. We need a function that forecasts continuous Y in the case of regression given X as independent features since we need to find the value of Y in regression.\n",
    "    \n",
    "Here, X is referred to as an independent variable as well as Y's predictor, while Y is referred to as the dependent or target variable. Regression can be done using a wide variety of modules or functions. The simplest sort of function is a linear function. Here, X could be one feature or a group of features that collectively represent the problem.\n",
    "    \n",
    "<img src=\"Lecture_Image_Files/lecture_15/linear-regression-plot.jpg\" width=\"300\">\n",
    "\n",
    "The task of predicting a dependent variable's value (y) based on a specified independent variable (x) is carried out using linear regression. Thus, the term \"linear regression\" was established. In the diagram above, X represents a person's job history and Y represents their wage. The line that fits our model the best is the regression line.\n",
    "\n",
    "Hypothesis function for Linear Regression :\n",
    "\n",
    "We assumed before that experience, or X, is our independent variable and that salary, or Y, is our dependent variable. Assuming that X and Y have a linear relationship, the salary may be predicted using:\n",
    "\n",
    "$ \\hat{Y} = \\theta_{1} + \\theta_{2}X $\n",
    "\n",
    "or,\n",
    "\n",
    "$ \\hat{y}_i = \\theta_{1} + \\theta_{2}x_i $\n",
    "\n",
    "Here,\n",
    "\n",
    "$ {y}_i \\in Y $ (for $ i = 1,2,\\ldots,n $) are true labels for the data.\n",
    "\n",
    "$ x_i \\in X $ (for $ i = 1,2,\\ldots,n $) are the input independent training data.\n",
    "\n",
    "$ \\hat{y}_i \\in \\hat{Y} $ (for $ i = 1,2,\\ldots,n $) are the predicted values. \n",
    "\n",
    "Finding the optimal θ1 and θ1 values allows the model to produce the best regression fit line.\n",
    "\n",
    "- θ1: intercept \n",
    "- θ2: coefficient of x \n",
    "\n",
    "The best-fit line is found after finding the best **θ1** and **θ1** values. Therefore, when we finally use our model to make a prediction, it will forecast the value of **y** based on the value of the input **x**. \n",
    " \n",
    "Cost function\n",
    "\n",
    "The cost function, often called the loss function, measures how well our predictions match the actual values. It quantifies the difference between the predicted value $\\hat{Y}$ and the true value $Y$. In simple terms, it helps us understand how far off our predictions are from reality. It is the **Mean Squared Error (MSE)** between the predicted value and the true value\n",
    "\n",
    "The cost function ($J$) can be written as:\n",
    "\n",
    "$$ J = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{Y}_i - Y_i)^2 $$\n",
    "\n",
    "Where:\n",
    "- $J$ is the cost function.\n",
    "- $\\hat{Y}_i$ is the predicted value for the i-th data point.\n",
    "- $Y_i$ is the true value for the i-th data point.\n",
    "- $n$ is the number of data points.\n",
    "\n",
    "### How to update θ1 and θ2 values to get the best-fit line? \n",
    "\n",
    "To find the best-fit regression line, the model tries to predict the target value $\\hat{Y}$ in a way that makes the difference between this prediction and the true value $Y$ as small as possible. So, it's important to adjust the $\\theta_1$ and $\\theta_2$ values to minimize the error between the predicted $y$ value (pred) and the true $y$ value ($y$).\n",
    "\n",
    "To minimize the cost function, you typically use an optimization technique like gradient descent. The equation to update the parameters $\\theta_1$ and $\\theta_2$ to minimize the cost function is:\n",
    "\n",
    "Minimize:\n",
    "\n",
    "$$ J(\\theta_1, \\theta_2) = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{Y}_i - Y_i)^2 $$\n",
    "\n",
    "Where:\n",
    "- $\\theta_1$ and $\\theta_2$ are the parameters to be adjusted to minimize the cost function.\n",
    "- $\\hat{Y}_i$ is the predicted value for the i-th data point.\n",
    "- $Y_i$ is the true value for the i-th data point.\n",
    "- $n$ is the number of data points.\n",
    "\n",
    "**Goal:** to minimize the cost function $J(\\theta_1, \\theta_2)$\n",
    "\n",
    "## Optimization Algorithms\n",
    "\n",
    "There are several alternatives to Gradient Descent. Popular ones are listed below:\n",
    "\n",
    "- L-BFGS\n",
    "- Levenberg-Marquardt Algorithm (LMA)\n",
    "- Simulated Annealing\n",
    "- evolutionary Algorithms (EA)\n",
    "- Particle Swarm Optimization (PSO).\n",
    "- Gradient Descent\n",
    "\n",
    "\n",
    "## We will use gradient descnet to optimize our function.\n",
    "\n",
    "### Gradient Descent: \n",
    "\n",
    "<img src=\"Lecture_Image_Files/lecture_15/gradient_vis.gif\" width=\"500\">\n",
    "\n",
    "By iteratively changing the model's parameters, a linear regression model can be trained using the method of optimization gradient descent in order to reduce the model's mean squared error (MSE) on training data. The model employs gradient descent to update **θ1** and **θ2** variables in order to decrease the Cost function (minimizing RMSE value) and obtain the best-fit line. Starting with random values for **θ1** and **θ2**, the goal is to iteratively update the values until the minimum cost is reached. \n",
    "\n",
    "A gradient is nothing more than a derivative that describes how a function behaves when its inputs vary slightly.\n",
    "\n",
    "Let’s differentiate the cost function(J) with respect to $ \\theta_1 $ :\n",
    "\n",
    "#### Derivative of Cost Function with Respect to θ₁\n",
    "\n",
    "<img src=\"Lecture_Image_Files/lecture_15/theta1.svg\" width=\"300\">\n",
    "\n",
    "#### Derivative of Cost Function with Respect to θ₂\n",
    "\n",
    "<img src=\"Lecture_Image_Files/lecture_15/theta2.svg\" width=\"300\">\n",
    "\n",
    "Linear regression aims to find the coefficients for a linear equation that best suits the training data. To update these coefficients, we follow the negative gradient of the Mean Squared Error. This helps us determine the appropriate values for both the intercept and the coefficient of **X**. The learning rate, denoted as $ \\alpha $, controls the size of the steps we take during this adjustment process.\n",
    "\n",
    "<img src=\"Lecture_Image_Files/lecture_15/gr_dsc.png\" width=\"500\">\n",
    "\n",
    "### Update Equation for Weight (Coefficient) in Linear Regression:\n",
    "\n",
    "The equation for updating the weight (or coefficient) in a gradient descent optimization process for linear regression is as follows:\n",
    "\n",
    "<img src=\"Lecture_Image_Files/lecture_15/update.svg\" width=\"500\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a16652",
   "metadata": {},
   "source": [
    "#### \"α\" is hyperparameter in this equation. Which is also called Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd0c513",
   "metadata": {},
   "source": [
    "## Gradient Descent Algorithm and Its Variants:\n",
    "\n",
    "# Batch Gradient Descent (BGD)\n",
    "- **Batch Size**: BGD uses the entire dataset for each iteration. It computes the gradient of the cost function with respect to the model parameters using all the training examples.\n",
    "- **Advantages**:\n",
    "  - It provides a more accurate estimate of the gradient since it considers the entire dataset.\n",
    "  - It usually converges to the global minimum, given enough time.\n",
    "- **Disadvantages**:\n",
    "  - It can be computationally expensive, especially for large datasets.\n",
    "  - It doesn't work well when the dataset doesn't fit into memory.\n",
    "\n",
    "# Stochastic Gradient Descent (SGD)\n",
    "- **Batch Size**: SGD uses a batch size of 1, meaning it updates the model parameters for each training example one at a time.\n",
    "- **Advantages**:\n",
    "  - It converges faster than BGD because it updates the model frequently.\n",
    "  - It can escape local minima more easily due to its noisy updates.\n",
    "- **Disadvantages**:\n",
    "  - The updates can be very noisy, leading to oscillations in the convergence process.\n",
    "  - It may not converge to the global minimum, and the final solution can be somewhat random.\n",
    "\n",
    "# Mini-Batch Gradient Descent\n",
    "- **Batch Size**: Mini-Batch Gradient Descent uses a batch size between 1 and the size of the entire dataset. Common batch sizes range from a few examples to a few hundred.\n",
    "- **Advantages**:\n",
    "  - It strikes a balance between the advantages of BGD and SGD. It provides a more stable convergence than SGD while being computationally efficient.\n",
    "  - It can take advantage of parallelism when computing gradients.\n",
    "- **Disadvantages**:\n",
    "  - The choice of batch size is a hyperparameter that needs tuning.\n",
    "  - The convergence behavior can be sensitive to the batch size and learning rate.\n",
    "\n",
    "In summary, the main differences between these gradient descent variants lie in the size of the batches used for computing gradients and updating model parameters. BGD uses the entire dataset, SGD uses a single example at a time, and Mini-Batch GD uses a small, fixed-size batch of examples. The choice of which variant to use depends on factors such as the dataset size, available computational resources, and desired convergence characteristics. Mini-Batch Gradient Descent is often the most practical choice for training deep learning models in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dcea8a",
   "metadata": {},
   "source": [
    "## Learning Rate in Machine Learning\n",
    "\n",
    "In machine learning and deep learning, the learning rate is a hyperparameter that plays a crucial role in controlling the step size at which a model's parameters (weights and biases) are updated during the training process. It determines how quickly or slowly a neural network learns from the data.\n",
    "\n",
    "### Gradient Descent Optimization\n",
    "\n",
    "- Gradient descent is a common optimization algorithm used to train machine learning and deep learning models.\n",
    "- During training, the goal is to minimize a loss function (also called a cost function) by iteratively updating the model's parameters.\n",
    "- The updates are made by moving in the direction of steepest descent, which is the direction of the negative gradient of the loss function.\n",
    "\n",
    "### The Role of Learning Rate\n",
    "\n",
    "- The learning rate (often denoted as \"α\" or \"η\") determines the step size taken in each iteration of gradient descent.\n",
    "- A larger learning rate means larger steps, which can make the training process faster but may lead to overshooting the optimal parameter values, causing the algorithm to diverge.\n",
    "- A smaller learning rate means smaller steps, which can lead to a more stable convergence but may result in slower training.\n",
    "\n",
    "### Choosing the Right Learning Rate\n",
    "\n",
    "<img src=\"Lecture_Image_Files/lecture_15/learning_rate_comp.png\" width=\"500\">\n",
    "\n",
    "- Selecting an appropriate learning rate is crucial for successful model training.\n",
    "- If the learning rate is too large, the optimization process may oscillate or diverge, preventing the model from converging to the optimal solution.\n",
    "- If the learning rate is too small, the optimization process may become extremely slow, and it may take a long time for the model to converge.\n",
    "\n",
    "### Techniques for Learning Rate Adjustment\n",
    "\n",
    "- Learning rate scheduling: This involves changing the learning rate during training, such as reducing it gradually over time (e.g., learning rate decay).\n",
    "- Adaptive learning rate methods: Algorithms like AdaGrad, RMSprop, and Adam adjust the learning rate dynamically based on the history of parameter updates to balance the step size.\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "\n",
    "- The choice of an appropriate learning rate often requires hyperparameter tuning, where different learning rates are tried, and the one that results in the best model performance on a validation set is selected.\n",
    "- Cross-validation and grid search are common techniques for hyperparameter tuning.\n",
    "\n",
    "In summary, the learning rate is a critical hyperparameter in the training of machine learning and deep learning models. It controls the step size for parameter updates during optimization and can significantly impact the training process, affecting convergence speed and the quality of the learned model. Finding the right learning rate is often a crucial part of the model training process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd80004b",
   "metadata": {},
   "source": [
    "## Vanishing vs Exploding Gradient\n",
    "\n",
    "In machine learning, the term **gradient** refers to the slope or rate of change of a loss function with respect to the model's weights. Gradients play a crucial role during backpropagation, where they are calculated and used to adjust the model's weights to minimize the loss function.\n",
    "\n",
    "### Common Issues During Backpropagation\n",
    "\n",
    "During the backpropagation process, two common issues can arise:\n",
    "\n",
    "#### Vanishing Gradient\n",
    "\n",
    "- **Definition:** Vanishing Gradient occurs when the gradient becomes extremely small as it propagates backward through the layers of a neural network.\n",
    "- **Causes:** It's a common challenge in deep neural networks, particularly when using activation functions like sigmoid or tanh. Deeper networks tend to suffer more, as gradients become progressively smaller during backpropagation.\n",
    "- **Consequences:** Early layers learn very slowly due to tiny gradients, hampering the training process.\n",
    "- **Solution:** Mitigate vanishing gradient by using activation functions like ReLU, which helps maintain more substantial gradients.\n",
    "\n",
    "#### Exploding Gradient\n",
    "\n",
    "- **Definition:** Exploding Gradient is the opposite issue, where gradients become excessively large during backpropagation, leading to unstable training. \n",
    "- **Causes:** Gradients explode when weights are updated dramatically or when activation functions allow unbounded growth.\n",
    "- **Consequences:** Unstable training can occur, and the loss function may become NaN or reach extremely high values.\n",
    "- **Solution:** Prevent gradient explosion by employing techniques such as weight initialization and gradient clipping.\n",
    "\n",
    "Addressing these gradient-related issues is crucial to ensure stable and effective training in neural networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05eb3723",
   "metadata": {},
   "source": [
    "## Convex Function:\n",
    "\n",
    "A convex function is a mathematical function that has a specific geometric property on its graph. In simple terms, a function is convex if, when you draw a straight line segment between any two points on its graph, that line segment lies entirely above or on the graph. In other words, the graph of a convex function curves upward or forms a \"bowl\" shape without any dips or local maxima (peaks).\n",
    "\n",
    "### Consequences of Non-Convex Functions in Machine Learning\n",
    "\n",
    "<img src=\"Lecture_Image_Files/lecture_15/global_local.png\" width=\"500\">\n",
    "\n",
    "When dealing with non-convex functions in machine learning, several challenges and complexities arise in the optimization process. Here are some key implications:\n",
    "\n",
    "1. **Multiple Local Optima:** Non-convex functions can have multiple local optima, making it difficult to find the global minimum—the lowest point across the entire domain. Optimization algorithms may converge to local minima instead.\n",
    "\n",
    "2. **Sensitivity to Initialization:** Different initial values for model parameters can lead to different local minima, making it challenging to ensure consistent and optimal results.\n",
    "\n",
    "3. **Convergence to Local Optima:** Gradient-based optimization algorithms may converge to local minima instead of the global minimum, resulting in suboptimal model performance.\n",
    "\n",
    "4. **Slower Convergence:** Non-convex functions often lead to slower convergence during training because optimization algorithms need to navigate complex landscapes with multiple peaks and valleys.\n",
    "\n",
    "5. **Overfitting Risk:** Non-convex optimization problems have a higher risk of overfitting, where the model fits the training data too closely, leading to poor generalization on unseen data.\n",
    "\n",
    "6. **Hyperparameter Tuning Challenge:** Tuning hyperparameters, such as learning rates, for non-convex functions can be more challenging as different regions of the function's landscape may require different settings.\n",
    "\n",
    "7. **Exploration Strategies:** Practitioners often employ various exploration strategies, including random initialization, multiple restarts, and heuristic techniques, to increase the chances of finding better solutions in non-convex optimization problems.\n",
    "\n",
    "8. **Computational Cost:** Searching for global optima in non-convex functions typically requires more computational resources and time compared to convex optimization.\n",
    "\n",
    "To mitigate these challenges, advanced optimization techniques, such as stochastic gradient descent with momentum, genetic algorithms, simulated annealing, and Bayesian optimization, are commonly used. Careful hyperparameter tuning and regularization methods can also enhance the robustness of the optimization process and improve the generalization performance of machine learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47027c4",
   "metadata": {},
   "source": [
    "## Vanishing Gradient vs. Gradient Explosion in Deep Neural Networks\n",
    "\n",
    "**Vanishing Gradient:**\n",
    "- **Definition:** Vanishing gradient occurs when gradients (derivatives of the loss function with respect to model parameters) become extremely small as they propagate backward through the layers of a deep neural network.\n",
    "- **Causes:** It often happens in deep networks with activation functions like sigmoid or hyperbolic tangent (tanh) that squash their inputs into a small range. Gradients diminish significantly during backpropagation, making it challenging for early layers to learn meaningful features.\n",
    "- **Consequences:** With vanishing gradients, early layers may learn slowly or not at all, hindering the network's ability to capture complex patterns.\n",
    "\n",
    "**Gradient Explosion:**\n",
    "- **Definition:** Gradient explosion is the opposite problem, where gradients become extremely large during backpropagation.\n",
    "- **Causes:** It can occur when weights are initialized with large values or when activation functions like ReLU allow unbounded growth of activations.\n",
    "- **Consequences:** Gradient explosion can lead to unstable training, causing the loss to become NaN or approach infinity, making training impractical.\n",
    "\n",
    "**Mitigation Techniques:**\n",
    "- **Weight Initialization:** Proper weight initialization, such as He initialization for ReLU or Xavier initialization for sigmoid and tanh, helps alleviate both issues.\n",
    "- **Activation Functions:** Using non-vanishing gradient activation functions like ReLU or Leaky ReLU can be beneficial.\n",
    "- **Batch Normalization:** Applying batch normalization between layers stabilizes training and mitigates both problems.\n",
    "- **Gradient Clipping:** Limiting gradient magnitude prevents gradient explosion.\n",
    "- **Residual Connections:** Skip or residual connections in deep architectures improve gradient flow.\n",
    "- **Gated Activation Functions:** Architectures like LSTM and GRU use gating mechanisms to control gradient flow, addressing vanishing gradient issues in recurrent neural networks.\n",
    "\n",
    "Balancing the trade-off between vanishing and exploding gradients is crucial when designing and training deep neural networks to ensure stable and effective learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7312a73c",
   "metadata": {},
   "source": [
    "## Parameters vs. Hyperparameters in Machine Learning\n",
    "\n",
    "In the realm of machine learning and deep learning, understanding the distinctions between \"parameters\" and \"hyperparameters\" is crucial as they play distinct roles within a model:\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- **Definition:** Parameters are internal variables that a machine learning or deep learning model learns from the training data. These variables act as coefficients or weights, determining the relationship between input features and the model's predicted output.\n",
    "\n",
    "- **Learned:** Parameters are learned automatically during the model training process. The model adjusts these parameters iteratively to minimize the difference between its predictions and the actual target values in the training dataset.\n",
    "\n",
    "- **Examples:** In a linear regression model, the coefficients associated with each feature are considered parameters. In a neural network, parameters include the weights and biases that connect neurons across different layers.\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "- **Definition:** Hyperparameters are external configurations or settings that are defined prior to training a machine learning or deep learning model. They control various aspects of the training process and influence the behavior of the model.\n",
    "\n",
    "- **Set by the User:** Hyperparameters are not learned from the data but are explicitly set by the user or the data scientist before initiating the training process. Proper tuning of hyperparameters is essential for optimizing model performance.\n",
    "\n",
    "- **Examples:** Common hyperparameters include the learning rate, batch size, the number of hidden layers in a neural network, the number of trees in a random forest, the strength of regularization techniques, and the choice of activation functions.\n",
    "\n",
    "In summary, parameters represent internal variables that the model learns from the data, whereas hyperparameters are external settings and configurations that users specify to control the training process and model behavior. Effective tuning of hyperparameters is vital for achieving optimal model performance, while parameters are learned by the model to make predictions based on the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e11e7e9",
   "metadata": {},
   "source": [
    "## Forward and Backward Propagation in Neural Networks\n",
    "\n",
    "Forward and backward propagation are fundamental concepts in training neural networks, particularly in the context of supervised learning. They are key processes that allow a neural network to learn from data and adjust its parameters (weights and biases) to make better predictions.\n",
    "\n",
    "### Forward Propagation:\n",
    "\n",
    "\n",
    "<img src=\"Lecture_Image_Files/lecture_15/forward.PNG\" width=\"500\">\n",
    "\n",
    "Forward propagation is the process by which an input is passed through the neural network to compute the predicted output (often referred to as the \"forward pass\"). It involves the following steps:\n",
    "\n",
    "1. **Input Layer**: The input data is fed into the neural network through the input layer. Each input feature corresponds to a neuron in the input layer.\n",
    "\n",
    "2. **Weights and Biases**: Each connection between neurons in one layer and neurons in the next layer is associated with a weight. Additionally, each neuron in a layer has an associated bias. These weights and biases are the parameters that the network learns during training.\n",
    "\n",
    "3. **Activation Function**: After calculating the weighted sum of inputs for each neuron in a layer, an activation function is applied to introduce non-linearity into the network. Common activation functions include ReLU (Rectified Linear Unit), Sigmoid, and Tanh.\n",
    "\n",
    "4. **Hidden Layers**: The input is passed through one or more hidden layers, with each layer performing weighted summation, applying activation functions, and passing the result to the next layer.\n",
    "\n",
    "5. **Output Layer**: The final hidden layer connects to the output layer. The output layer produces the predicted values or probabilities, depending on the type of problem (e.g., classification or regression).\n",
    "\n",
    "6. **Loss Calculation**: The predicted output is compared to the actual target values to calculate a loss or cost. The choice of loss function depends on the specific problem (e.g., Mean Squared Error for regression, Cross-Entropy for classification).\n",
    "\n",
    "### Backward Propagation (Backpropagation):\n",
    "\n",
    "\n",
    "<img src=\"Lecture_Image_Files/lecture_15/backward.png\" width=\"500\">\n",
    "\n",
    "Backward propagation, often simply referred to as \"backpropagation,\" is the process by which the neural network learns from its mistakes by adjusting its parameters. It involves the following steps:\n",
    "\n",
    "1. **Gradient Calculation**: The gradient of the loss with respect to each weight and bias in the network is calculated. This gradient represents the direction and magnitude of the change required to minimize the loss.\n",
    "\n",
    "2. **Error Backpropagation**: The gradient is propagated backward through the network, starting from the output layer and moving toward the input layer. This step involves applying the chain rule of calculus to calculate gradients at each layer.\n",
    "\n",
    "3. **Parameter Update**: The gradients are used to update the network's parameters (weights and biases) using an optimization algorithm such as Gradient Descent. The learning rate determines the step size for these updates.\n",
    "\n",
    "4. **Iterative Process**: Steps 1 to 3 are repeated iteratively for a specified number of epochs or until convergence. This process allows the network to gradually reduce the loss and improve its predictions.\n",
    "\n",
    "### Importance:\n",
    "\n",
    "- Forward propagation is responsible for making predictions given the current network parameters.\n",
    "\n",
    "- Backward propagation is essential for training the network. It computes the gradients that guide parameter updates to minimize the loss function.\n",
    "\n",
    "Together, forward and backward propagation enable neural networks to learn from data, adjust their parameters to minimize errors, and make accurate predictions on new, unseen data. This process is known as supervised learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc7333f",
   "metadata": {},
   "source": [
    "## Iteration vs. Epoch in Machine Learning\n",
    "\n",
    "In machine learning, we use terms like \"epochs,\" \"batch size,\" and \"iterations\" mainly when dealing with large datasets, which is quite common. When our data is too big to be processed all at once by the computer, we need to break it into smaller portions and feed them gradually to the neural network. We update the neural network's weights at the end of each step to make it better fit the given data.\n",
    "\n",
    "## Epochs:\n",
    "An epoch is when we go through the entire dataset once, both forward and backward, in the neural network. Because it's impractical to process the entire dataset at once, we split it into smaller batches.\n",
    "\n",
    "Why We Use Multiple Epochs: Initially, it might seem odd that we have to pass the same data through the neural network multiple times. However, it's important to remember that we're working with a limited dataset, and we're using an iterative process called Gradient Descent to improve the model. Updating the weights with just a single pass (one epoch) isn't sufficient for effective learning and optimization.\n",
    "\n",
    "One epoch leads to underfitting of the curve in the graph (below):\n",
    "\n",
    "<img src=\"Lecture_Image_Files/lecture_15/overfit_under.png\" width=\"500\">\n",
    "\n",
    "When we increase the number of epochs, it means we're allowing the neural network to adjust its weights more times. This process changes the performance curve of the model: at first, it's not fitting well, then it gets better, but if we go too far, it might start fitting too much, which is not good.\n",
    "\n",
    "\n",
    "Determining the right number of epochs is a task without a fixed answer. It varies from one dataset to another. You could say that the number of epochs relates to the diversity of your data. For instance, consider whether your dataset contains only black cats or if it's much more varied in terms of content.\n",
    "\n",
    "## Batch Size in Machine Learning\n",
    "\n",
    "In machine learning, \"batch size\" refers to the total number of training examples included in a single batch during the training of a model. It's important to note that batch size and the number of batches are distinct concepts:\n",
    "\n",
    "### Batch Size\n",
    "\n",
    "- **Definition:** Batch size specifies how many data points are processed together in each iteration during model training. A larger batch size means that more examples are considered simultaneously, which can speed up training but might require more memory.\n",
    "\n",
    "### Number of Batches\n",
    "\n",
    "- **Definition:** The number of batches is determined by dividing the total number of training examples by the batch size. It represents how many groups of data are processed during one epoch (a complete pass through the entire training dataset).\n",
    "\n",
    "- **Example:** If you have 1,000 training examples and a batch size of 100, you would have 10 batches in one epoch (1,000 / 100).\n",
    "\n",
    "In summary, batch size controls how many training examples are processed at once in each iteration, while the number of batches represents the divisions of your training dataset into these groups for training over the course of one epoch.\n",
    "\n",
    "## Iterations in Machine Learning\n",
    "\n",
    "Iterations in machine learning refer to the number of batches required to complete one full epoch of training. Calculating the number of iterations is a straightforward process, typically involving the total number of training examples and the chosen batch size. Here's a simplified explanation:\n",
    "\n",
    "### Iterations\n",
    "\n",
    "- **Definition:** Iterations represent the count of batches needed to process the entire training dataset once, which constitutes one epoch of training.\n",
    "\n",
    "- **Calculation:** To determine the number of iterations needed for one epoch, you divide the total number of training examples by the chosen batch size.\n",
    "\n",
    "### Example\n",
    "\n",
    "Let's illustrate this concept with an example:\n",
    "\n",
    "- Suppose you have a dataset of 2,000 training examples.\n",
    "- You decide to use a batch size of 500.\n",
    "- To complete one full epoch of training, you perform 2,000 / 500, which equals 4 iterations.\n",
    "- Therefore, for one complete epoch, you need 4 iterations, with each iteration processing a batch of 500 examples.\n",
    "\n",
    "This approach efficiently handles the training of machine learning models, especially when working with large datasets, by dividing them into manageable batches for processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa5762e",
   "metadata": {},
   "source": [
    "### Build the Linear Regression model from Scratch\n",
    "\n",
    "Import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fc154b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.axes as ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8251aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/lec_15/data_for_lr.csv')\n",
    "\n",
    "# Drop the missing values\n",
    "data = data.dropna()\n",
    "\n",
    "# training dataset and labels\n",
    "train_input = np.array(data.x[0:500]).reshape(500,1)\n",
    "train_output  = np.array(data.y[0:500]).reshape(500,1)\n",
    " \n",
    "# valid dataset and labels\n",
    "test_input = np.array(data.x[500:700]).reshape(199,1)\n",
    "test_output  = np.array(data.y[500:700]).reshape(199,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17faa7e7",
   "metadata": {},
   "source": [
    "### Steps to Create a Linear Regression Model:\n",
    "\n",
    "- Initially providing a random value to the parameters (m & c), the linear regression function Y=mx+x is applied in forward propagation.\n",
    "- The function for calculating the cost function, or the mean, has been written by us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63b121f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self):\n",
    "        self.parameters = {}\n",
    "     \n",
    "    def forward_propagation(self, train_input):\n",
    "        m = self.parameters['m']\n",
    "        c = self.parameters['c']\n",
    "        predictions = np.multiply(m, train_input) + c\n",
    "        return predictions\n",
    " \n",
    "    def cost_function(self, predictions, train_output):\n",
    "        cost = np.mean((train_output - predictions) ** 2)\n",
    "        return cost\n",
    " \n",
    "    def backward_propagation(self, train_input, train_output, predictions):\n",
    "        derivatives = {}\n",
    "        df = (train_output - predictions) * -1\n",
    "        dm = np.mean(np.multiply(train_input, df))\n",
    "        dc = np.mean(df)\n",
    "        derivatives['dm'] = dm\n",
    "        derivatives['dc'] = dc\n",
    "        return derivatives\n",
    " \n",
    "    def update_parameters(self, derivatives, learning_rate):\n",
    "        self.parameters['m'] = self.parameters['m'] - learning_rate * derivatives['dm']\n",
    "        self.parameters['c'] = self.parameters['c'] - learning_rate * derivatives['dc']\n",
    " \n",
    "    def train(self, train_input, train_output, learning_rate, iters):\n",
    "        #initialize random parameters\n",
    "        self.parameters['m'] = np.random.uniform(0,1) * -1\n",
    "        self.parameters['c'] = np.random.uniform(0,1) * -1\n",
    "         \n",
    "        #initialize loss\n",
    "        self.loss = []\n",
    "         \n",
    "        #iterate\n",
    "        for i in range(iters):\n",
    "            #forward propagation\n",
    "            predictions = self.forward_propagation(train_input)\n",
    " \n",
    "            #cost function\n",
    "            cost = self.cost_function(predictions, train_output)\n",
    " \n",
    "            #append loss and print\n",
    "            self.loss.append(cost)\n",
    "            print(\"Iteration = {}, Loss = {}\".format(i+1, cost))\n",
    " \n",
    "            #back propagation\n",
    "            derivatives = self.backward_propagation(train_input, train_output, predictions)\n",
    " \n",
    "            #update parameters\n",
    "            self.update_parameters(derivatives, learning_rate)\n",
    " \n",
    "        return self.parameters, self.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a872fd19",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cd793b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example usage\n",
    "linear_reg = LinearRegression()\n",
    "parameters, loss = linear_reg.train(train_input, train_output, 0.0001, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99badf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction on test data\n",
    "y_pred = test_input*parameters['m'] + parameters['c']\n",
    " \n",
    "# Plot the regression line with actual data pointa\n",
    "plt.plot(test_input, test_output, '+', label='Actual values')\n",
    "plt.plot(test_input, y_pred, label='Predicted values')\n",
    "plt.xlabel('Test input')\n",
    "plt.ylabel('Test Output or Predicted output')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1640acb",
   "metadata": {},
   "source": [
    "# Image Classification\n",
    "\n",
    "Image classification is a specific application of classification where the input data is in the form of images. The goal is to assign one or more labels to an image based on its content or characteristics. Deep learning methods, particularly convolutional neural networks (CNNs), have proven to be highly effective in image classification tasks.\n",
    "\n",
    "In image classification, the input image is typically represented as a matrix of pixel values. Deep learning models learn hierarchical features from the images, starting from simple features like edges and textures and gradually capturing more complex patterns. The model outputs a probability distribution over all possible classes, indicating the likelihood of the image belonging to each class. The class with the highest probability is assigned as the predicted label for the image.\n",
    "\n",
    "The equations used in image classification depend on the specific architecture and approach employed, such as convolutional layers, pooling layers, and fully connected layers in a CNN. These equations involve numerous matrix multiplications, non-linear activation functions, and other operations to extract and process features from the image data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e73d560",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Logistic regression is a learning algorithm used in a supervised learning problem when the output $y$ are all either zero or one. The goal of logistic regression is to minimize the error between its predictions and training data.\n",
    "\n",
    "## Example: Cat vs No - Cat\n",
    "\n",
    "Given an image represented by a feature vector $x$, the algorithm will evaluate the probability of a cat being in that image.\n",
    "\n",
    "$G(x, \\hat{y}) = P(y = 1|x)$, where $0 \\leq \\hat{y} \\leq 1$\n",
    "\n",
    "## Parameters of Logistic Regression\n",
    "\n",
    "The parameters used in Logistic regression are:\n",
    "\n",
    "- The input features vector: $x \\in \\mathbb{R}^{n_x}$, where $n_x$ is the number of features\n",
    "- The training label: $y \\in \\{0, 1\\}$\n",
    "- The weights: $w \\in \\mathbb{R}^{n_x}$, where $n_x$ is the number of features\n",
    "- The threshold: $b \\in \\mathbb{R}$\n",
    "- The output: $\\hat{y} = \\sigma(w^T x + b)$\n",
    "\n",
    "## Sigmoid Function\n",
    "\n",
    "The sigmoid function, $\\sigma(z)$, is used in logistic regression to constrain the output between 0 and 1:\n",
    "\n",
    "<img src='Lecture_Image_Files/lecture_15/sig.png' width=\"500\"/>\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "- If $z$ is a large positive number, then $\\sigma(z) = 1$.\n",
    "- If $z$ is a small or large negative number, then $\\sigma(z) = 0$.\n",
    "- If $z = 0$, then $\\sigma(z) = 0.5$.\n",
    "\n",
    "The sigmoid function is chosen because it maps the linear combination $w^T x + b$ to a probability range [0, 1].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d626df2",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a8d835",
   "metadata": {},
   "source": [
    "### Perceptron in Deep Learning\n",
    "\n",
    "<img src='Lecture_Image_Files/lecture_15/perceptron.png' width=\"500\"/>\n",
    "\n",
    "A **Perceptron** is the fundamental building block of a neural network in deep learning. It is a simple algorithm designed for binary classification tasks. The operation of a perceptron can be described with the following steps:\n",
    "\n",
    "1. **Input:** A perceptron takes several inputs, which we denote as `x1, x2, ..., xn`. Each of these inputs has an associated weight, denoted as `w1, w2, ..., wn`. The weights are learned parameters that the perceptron adjusts to improve its predictions.\n",
    "\n",
    "2. **Weighted Sum:** Each input `xi` is multiplied by its corresponding weight `wi`, and these products are summed together. Additionally, a bias term `b` is added. The bias is another learned parameter that allows the perceptron to make better predictions. This step is represented by the equation:\n",
    "\n",
    "    ```\n",
    "    z = w1*x1 + w2*x2 + ... + wn*xn + b\n",
    "    ```\n",
    "\n",
    "    Here, `z` is the weighted sum plus the bias.\n",
    "\n",
    "3. **Activation Function:** The weighted sum `z` is then passed through an activation function. In the original perceptron algorithm, this is a step function that outputs 1 if the input is greater than a certain threshold, and -1 otherwise. We denote the activation function as `f`, and the output of the perceptron as `y`. This step is represented by the equation:\n",
    "\n",
    "    ```\n",
    "    y = f(z)\n",
    "    ```\n",
    "\n",
    "    Here, `f` is the step function. A common choice of step function is:\n",
    "\n",
    "    ```\n",
    "    f(z) = 1 if z > 0, else -1\n",
    "    ```\n",
    "\n",
    "In summary, the operation of a perceptron can be represented by these two equations:\n",
    "```\n",
    "z = w1x1 + w2x2 + ... + wn*xn + b\n",
    "y = f(z)\n",
    "```\n",
    "\n",
    "The perceptron learns the optimal weights and bias by iteratively updating these parameters in response to the prediction errors it makes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af243ac4",
   "metadata": {},
   "source": [
    "# Perceptron for Binary Classification using pytorch\n",
    "\n",
    "In this example, we will demonstrate how to create a simple Perceptron model for binary classification using PyTorch, and how to visualize training and testing accuracy.\n",
    "\n",
    "<img src='Lecture_Image_Files/lecture_15/per_binary.jpg' width=\"500\"/>\n",
    "\n",
    "First, we start by importing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2609db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data from the CSV file into a Pandas DataFrame\n",
    "csv_file = \"data/lec_15/movie_decision_data.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Convert DataFrame columns to the appropriate data types\n",
    "df['Weather'] = df['Weather'].astype('float32')\n",
    "df['Proximity'] = df['Proximity'].astype('float32')\n",
    "df['Company'] = df['Company'].astype('float32')\n",
    "df['Decision'] = df['Decision'].astype('float32')\n",
    "\n",
    "# Separate input features (X) and labels (y)\n",
    "X = torch.tensor(df[['Weather', 'Proximity', 'Company']].values, dtype=torch.float32)\n",
    "y = torch.tensor(df['Decision'].values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the Perceptron model\n",
    "class Perceptron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(3, 1)  # 3 input features, 1 output neuron (binary classification)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc(x))\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = Perceptron()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Lists to store training history for plotting\n",
    "loss_history = []\n",
    "accuracy_history = []\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    predicted_labels = (outputs > 0.5).float()\n",
    "    correct_predictions = (predicted_labels == y_train).sum().item()\n",
    "    accuracy = correct_predictions / len(y_train)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_history.append(loss.item())\n",
    "    accuracy_history.append(accuracy)\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Make predictions with the trained model on the test dataset\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test)\n",
    "    test_predicted_labels = (test_outputs > 0.5).float()\n",
    "\n",
    "# Calculate accuracy on the test dataset\n",
    "test_accuracy = accuracy_score(y_test.numpy(), test_predicted_labels.numpy())\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "# Plot training loss and accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs + 1), loss_history, label='Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs + 1), accuracy_history, label='Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training Accuracy')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3c098b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the parameters\n",
    "for i in model.parameters():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbea0e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count parameters\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffe6e63",
   "metadata": {},
   "source": [
    "### Predict on user data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9577de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input from the user\n",
    "print(\"Enter the weather (0 or 1):\")\n",
    "weather = float(input())\n",
    "print(\"Enter the proximity (0 or 1):\")\n",
    "proximity = float(input())\n",
    "print(\"Enter the company (0 or 1):\")\n",
    "company = float(input())\n",
    "\n",
    "# Prepare the input tensor\n",
    "user_input = torch.tensor([weather, proximity, company], dtype=torch.float32).view(1, -1)\n",
    "\n",
    "# Make predictions with the trained model\n",
    "with torch.no_grad():\n",
    "    prediction = model(user_input)\n",
    "\n",
    "# Determine the decision based on the prediction\n",
    "decision = \"Go to watch\" if prediction.item() > 0.5 else \"Stay at home\"\n",
    "print(f\"Decision: {decision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1b6188",
   "metadata": {},
   "source": [
    "\n",
    "# Multi-Layer Perceptron\n",
    "\n",
    "MLP:\n",
    "<img src='Lecture_Image_Files/lecture_15/mlp.png' width=\"500\"/>\n",
    "\n",
    "Perceptron VS MLP:\n",
    "\n",
    "<img src='Lecture_Image_Files/lecture_15/per_mlp.jpg' width=\"500\"/>\n",
    "\n",
    "\n",
    "A **Multi-Layer Perceptron (MLP)** is a type of artificial neural network that is built from multiple layers of perceptrons. MLPs are designed to model more complex patterns by introducing additional layers and non-linear activation functions, allowing them to solve problems that single-layer perceptrons cannot.\n",
    "\n",
    "A MLP consists of at least three layers:\n",
    "\n",
    "- An **input layer**, which corresponds to the input features.\n",
    "- One or more **hidden layers**, which transform the inputs into something that the output layer can use.\n",
    "- An **output layer**, which makes the final prediction.\n",
    "\n",
    "Each layer in an MLP is fully connected to the next layer, meaning each node in a layer connects with a weight to every node in the following layer.\n",
    "\n",
    "The operation of an MLP is similar to that of a single-layer perceptron, but repeated across multiple layers:\n",
    "\n",
    "1. The inputs are multiplied by the weights and summed together with the biases, similar to the operation of a single-layer perceptron. This is done for each node in the first hidden layer.\n",
    "\n",
    "    ```\n",
    "    z1 = w11*x1 + w12*x2 + ... + w1n*xn + b1\n",
    "    ```\n",
    "\n",
    "    This operation is repeated for all nodes in the hidden layer.\n",
    "\n",
    "2. The results are passed through a non-linear activation function. While the step function was traditionally used in single-layer perceptrons, MLPs often use other activation functions like ReLU, sigmoid or tanh to introduce non-linearity. \n",
    "\n",
    "    ```\n",
    "    h1 = f(z1)\n",
    "    ```\n",
    "\n",
    "    Here, `h1` is the output of the first hidden layer after applying the activation function `f`. This process is repeated for each hidden layer in the MLP.\n",
    "\n",
    "3. The outputs of the final hidden layer are used as inputs to the output layer, where the process is repeated to generate the final predictions.\n",
    "\n",
    "The primary difference between a single-layer perceptron and an MLP is the introduction of one or more hidden layers and the use of non-linear activation functions. These additions make MLPs capable of modeling more complex relationships in the data. Like single-layer perceptrons, MLPs learn by iteratively adjusting the weights and biases in response to the prediction errors they make.\n",
    "\n",
    "<img src='Lecture_Image_Files/lecture_15/nn_representation.png' width=\"500\"/>\n",
    "\n",
    "<img src='Lecture_Image_Files/lecture_15/nn_rep_1.png' width=\"500\"/>\n",
    "\n",
    "<img src='Lecture_Image_Files/lecture_15/nn_rep_2.jpg' width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2ea0f9",
   "metadata": {},
   "source": [
    "### 2. Implementaion of  MLP using pytorch\n",
    "\n",
    "* #### In this implementation we will use dummy input rather than using any real input data. Then we will work our way up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6b12bc",
   "metadata": {},
   "source": [
    "#### Before running the code install the necessary package.\n",
    "\n",
    "To install matplotlib.\n",
    "\n",
    "` Run: \n",
    "!pip install matplotlib \n",
    "\n",
    "        !pip install pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45ed3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4afe523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b76684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### SETTINGS\n",
    "##########################\n",
    "\n",
    "RANDOM_SEED = 1\n",
    "BATCH_SIZE = 100\n",
    "NUM_EPOCHS = 5\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "##########################\n",
    "### MNIST DATASET\n",
    "##########################\n",
    "\n",
    "# Note transforms.ToTensor() scales input images\n",
    "# to 0-1 range\n",
    "train_dataset = datasets.MNIST(root='data', \n",
    "                               train=True, \n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='data', \n",
    "                              train=False, \n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=BATCH_SIZE, \n",
    "                          shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=BATCH_SIZE, \n",
    "                         shuffle=False)\n",
    "\n",
    "# Checking the dataset\n",
    "for images, labels in train_loader:  \n",
    "    print('Image batch dimensions:', images.shape)\n",
    "    print('Image label dimensions:', labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cf326c",
   "metadata": {},
   "outputs": [],
   "source": [
    "myit = iter(train_loader)\n",
    "\n",
    "images, classes = next(myit)\n",
    "images, classes = next(myit)\n",
    "\n",
    "print(images.shape)\n",
    "print(classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf0c90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display images from the dataloader\n",
    "plt.imshow(images[1].reshape(28,28), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edc0596",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.squeeze(images[1])\n",
    "\n",
    "fig = plt.figure(figsize = (12,12)) \n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(img, cmap='gray')\n",
    "width, height = img.shape\n",
    "thresh = img.max()/2.5\n",
    "for x in range(width):\n",
    "    for y in range(height):\n",
    "        val = round(img[x][y].tolist(),2) if img[x][y] !=0 else 0\n",
    "        ax.annotate(str(val), xy=(y,x),\n",
    "                    horizontalalignment='center',\n",
    "                    verticalalignment='center',\n",
    "                    color='white' if img[x][y]<thresh else 'black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd48b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_features, num_hidden, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        ### 1st hidden layer\n",
    "        self.linear_1 = torch.nn.Linear(num_features, num_hidden)\n",
    "\n",
    "        ### Output layer\n",
    "        self.linear_out = torch.nn.Linear(num_hidden, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear_1(x)\n",
    "        out = torch.sigmoid(out)\n",
    "        logits = self.linear_out(out)\n",
    "        #probas = torch.softmax(logits, dim=1)\n",
    "        return logits#, probas\n",
    "\n",
    "    \n",
    "#################################\n",
    "### Model Initialization\n",
    "#################################\n",
    "    \n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "model = MLP(num_features=28*28,\n",
    "            num_hidden=100,\n",
    "            num_classes=10)\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "#################################\n",
    "### Training\n",
    "#################################\n",
    "\n",
    "def compute_loss(net, data_loader):\n",
    "    curr_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for cnt, (features, targets) in enumerate(data_loader):\n",
    "            features = features.view(-1, 28*28).to(DEVICE)\n",
    "            targets = targets.to(DEVICE)\n",
    "            logits = net(features)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            curr_loss += loss\n",
    "        return float(curr_loss)/cnt\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "minibatch_cost = []\n",
    "epoch_cost = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        \n",
    "        features = features.view(-1, 28*28).to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "            \n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits = model(features)\n",
    "        \n",
    "        cost = F.cross_entropy(logits, targets)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "       \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        minibatch_cost.append(cost.item())\n",
    "        if not batch_idx % 50:\n",
    "            print ('Epoch: %03d/%03d | Batch %03d/%03d | Cost: %.4f' \n",
    "                   %(epoch+1, NUM_EPOCHS, batch_idx, \n",
    "                     len(train_loader), cost.item()))\n",
    "        \n",
    "    cost = compute_loss(model, train_loader)\n",
    "    epoch_cost.append(cost)\n",
    "    print('Epoch: %03d/%03d Train Cost: %.4f' % (\n",
    "            epoch+1, NUM_EPOCHS, cost))\n",
    "    print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "    \n",
    "print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee8ccc7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(range(len(minibatch_cost)), minibatch_cost)\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.xlabel('Minibatch')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(len(epoch_cost)), epoch_cost)\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733fb8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader):\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for features, targets in data_loader:\n",
    "            features = features.view(-1, 28*28).to(DEVICE)\n",
    "            targets = targets.to(DEVICE)\n",
    "            logits = model.forward(features)\n",
    "            predicted_labels = torch.argmax(logits, 1)\n",
    "            num_examples += targets.size(0)\n",
    "            correct_pred += (predicted_labels == targets).sum()\n",
    "        return correct_pred.float()/num_examples * 100\n",
    "    \n",
    "print('Test Accuracy: %.2f' % compute_accuracy(model, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fbd3b0",
   "metadata": {},
   "source": [
    "### Activation Functions in Neural Networks (Sigmoid, tanh, Softmax, ReLU, Leaky ReLU )\n",
    "\n",
    "#### What is Activation Function?\n",
    "\n",
    "It is simply a function that you use to obtain a node's output. It also goes by the name Transfer Function.\n",
    "\n",
    "#### Why we use Activation functions with Neural Networks?\n",
    "In neural networks, an activation function serves the essential role of introducing non-linearity into the model.\n",
    "\n",
    "Neural networks consist of multiple nodes or neurons in each layer, and in a fully-connected network, these neurons are interconnected. Let's focus on how we calculate the value of the first neuron in the second layer. Each neuron in the first layer is multiplied by a weight, where these weights are learned during training. The resulting products are summed together, and then a bias is added to this sum. However, if we were to limit ourselves to just these multiplications and additions, our models would be restricted to learning only simple, linear relationships.\n",
    "\n",
    "To enable the learning of complex patterns and relationships, we must break away from linearity. This is where activation functions come into play. Activation functions are non-linear functions applied immediately after adding the bias. They transform the output, providing the neuron (in our example, the first neuron in the second layer) with a non-linear value, allowing the network to capture intricate and non-linear relationships within the data.\n",
    "\n",
    "The two main categories of activation functions are:\n",
    "- Linear Activation Function\n",
    "- Non-Linear Activation Functions \n",
    "\n",
    "#### Linear or Identity Activation Function:\n",
    "\n",
    "<img src='Lecture_Image_Files/lecture_15/linear.jpg' width=\"500\"/>\n",
    "\n",
    "The function is a line or linear, as you can see. Thus, no range will be used to limit the output of the functions.\n",
    "\n",
    "Equation : f(x) = x\n",
    "\n",
    "Range : (-infinity to infinity)\n",
    "\n",
    "It doesn’t help with the complexity or various parameters of usual data that is fed to the neural networks.\n",
    "\n",
    "Non-linear Activation Function:\n",
    "\n",
    "The most frequently utilized activation functions are nonlinear activation functions. The graph appears like this because of nonlinearity.\n",
    "\n",
    "<img src='Lecture_Image_Files/lecture_15/non_linear.png' width=\"500\"/>\n",
    "\n",
    "Activation functions help the neural network handle different types of data and make it better at recognizing and distinguishing different outputs.\n",
    "\n",
    "The Nonlinear Activation Functions are mainly divided on the basis of their range or curves:\n",
    "\n",
    "### 1. Sigmoid or Logistic Activation Function\n",
    "\n",
    "The Sigmoid Function curve looks like a S-shape.\n",
    "\n",
    "<img src='Lecture_Image_Files/lecture_15/sigmoid.png' width=\"500\"/>\n",
    "\n",
    "The sigmoid function is represented by the following mathematical equation: \n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "In this equation:\n",
    "- $ \\sigma(x) $ represents the sigmoid function applied to the input value $ x $.\n",
    "- $ e $ is the base of the natural logarithm (approximately 2.71828).\n",
    "- The function takes the input $ x $, applies the exponential function to it ($ e^{-x} $), and then computes $ \\frac{1}{1 + e^{-x}} $, resulting in a value between 0 and 1.\n",
    "\n",
    "<img src='Lecture_Image_Files/lecture_15/sigmoidUnit.png' width=\"500\"/>\n",
    "\n",
    "We chose the sigmoid function primarily because the range is between 0 and 1. As a result, it is particularly used for models whose output is a probability prediction.The sigmoid is the best option because anything has a probability that only occurs between 0 and 1.\n",
    "\n",
    "The function is differentiable.That means, we can find the slope of the sigmoid curve at any two points.\n",
    "\n",
    "The softmax function is a more generalized logistic activation function which is used for multiclass classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009cea6f",
   "metadata": {},
   "source": [
    "## Code for sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e50c307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Dummy input\n",
    "dummy_input = np.array([-1, 0, 1, 2])\n",
    "\n",
    "# Apply sigmoid function to the dummy input\n",
    "result = sigmoid(dummy_input)\n",
    "\n",
    "# Print the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fd91f5",
   "metadata": {},
   "source": [
    "### 2. Tanh or hyperbolic tangent Activation Function\n",
    "\n",
    "Tanh is like a better version of a logistic sigmoid. The tanh function has a range of (-1 to 1). Tanh also has an s-shaped sigmoidal form.\n",
    "\n",
    "<img src='Lecture_Image_Files/lecture_15/sig_vs_tanh.png' width=\"500\"/>\n",
    "\n",
    "The positive aspect of this is that the zero inputs will be mapped near zero and the negative inputs will be highly negative in the tanh graph.\n",
    "\n",
    "The tanh function is mainly used classification between two classes.\n",
    "\n",
    "Both tanh and logistic sigmoid activation functions are used in feed-forward nets.\n",
    "\n",
    "Equation for Tanh is:\n",
    "$ \\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8f0737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the tanh activation function\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "# Dummy input\n",
    "dummy_input = np.array([-1, 0, 1, 2])\n",
    "\n",
    "# Apply tanh function to the dummy input\n",
    "result = tanh(dummy_input)\n",
    "\n",
    "# Print the result\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd63b49",
   "metadata": {},
   "source": [
    "### 3. ReLU (Rectified Linear Unit) Activation Function\n",
    "\n",
    "ReLU is the most popular activation function in the world right now because it's used in nearly all convolutional neural networks and deep learning models. It's widely adopted because of its effectiveness.\n",
    "<img src='Lecture_Image_Files/lecture_15/sig_relu.png' width=\"500\"/>\n",
    "\n",
    "The Rectified Linear Unit (ReLU) activation function is represented by the following mathematical equation:\n",
    "\n",
    "$\n",
    "\\text{ReLU}(x) = \\left\\{\n",
    "  \\begin{array}{ll}\n",
    "    x & \\text{if } x > 0 \\\\\n",
    "    0 & \\text{if } x \\leq 0\n",
    "  \\end{array}\n",
    "\\right.\n",
    "$\n",
    "\n",
    "In this equation:\n",
    "- $\\text{ReLU}(x)$ represents the ReLU activation function applied to the input value $x$.\n",
    "- If the input $x$ is greater than 0, the ReLU function keeps it as is ($x$).\n",
    "- If the input $x$ is less than or equal to 0, the ReLU function changes it to 0.\n",
    "\n",
    "Range: [ 0 to infinity)\n",
    "\n",
    "The problem with the ReLU activation function is that it turns all negative values into zeros right away. This can limit the model's ability to learn from the data effectively because it doesn't handle negative values well and doesn't capture certain patterns in the data accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d821861b",
   "metadata": {},
   "source": [
    "### Code for Relu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e31d3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the ReLU activation function\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Dummy input\n",
    "dummy_input = np.array([-1, 0, 1, 2])\n",
    "\n",
    "# Apply ReLU function to the dummy input\n",
    "result = relu(dummy_input)\n",
    "\n",
    "# Print the result\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4913132",
   "metadata": {},
   "source": [
    "### 4. Leaky ReLU\n",
    "\n",
    "It is an attempt to solve the dying ReLU problem\n",
    "\n",
    "<img src='Lecture_Image_Files/lecture_15/leaky_relu.png' width=\"500\"/>\n",
    "\n",
    "As you can see in the figure there is a leak. \n",
    "\n",
    "The leaky ReLU function expands the range of the ReLU function. Typically, it has a small positive value for 'a,' like 0.01. When 'a' is not 0.01, it's called Randomized ReLU. As a result, the leaky ReLU has a wider output range from negative infinity to positive infinity.\n",
    "\n",
    "\n",
    "The Leaky Rectified Linear Unit (Leaky ReLU) activation function is represented by the following mathematical equation:\n",
    "\n",
    "$\n",
    "\\text{LeakyReLU}(x) = \\begin{cases} \n",
    "      x & \\text{if } x > 0 \\\\\n",
    "      ax & \\text{if } x \\leq 0 \n",
    "   \\end{cases}\n",
    "$\n",
    "\n",
    "In this equation:\n",
    "- $\\text{LeakyReLU}(x)$ represents the Leaky ReLU activation function applied to the input value $x$.\n",
    "- If the input $x$ is greater than 0, the Leaky ReLU function keeps it as is ($x$).\n",
    "- If the input $x$ is less than or equal to 0, the Leaky ReLU function multiplies it by a small positive value 'a,' typically around 0.01.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e74832",
   "metadata": {},
   "source": [
    "### Code for leaky Relu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9e6b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the Leaky ReLU activation function\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "# Dummy input\n",
    "dummy_input = np.array([-1, 0, 1, 2])\n",
    "\n",
    "# Apply Leaky ReLU function to the dummy input (default alpha=0.01)\n",
    "result = leaky_relu(dummy_input)\n",
    "\n",
    "# Print the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7471cf0",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "In the code above:\n",
    "\n",
    "The softmax function takes an input vector **x** where each row represents a set of scores or logits for different classes.\n",
    "\n",
    "It computes the exponentials of the input vector to obtain the unnormalized probabilities.\n",
    "\n",
    "Then, it calculates the sum of exponentials along each row to normalize the probabilities for each class.\n",
    "\n",
    "Finally, it returns the softmax probabilities for each class.\n",
    "\n",
    "You can use this **softmax** function to transform the raw scores or logits from your neural network into probabilities. The resulting **softmax_output** is a probability distribution over the classes for each input sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c5cc4f",
   "metadata": {},
   "source": [
    "# Loss Functions\n",
    "\n",
    "A loss function, also known as a cost function or objective function, is a mathematical measure that quantifies the discrepancy between the predicted output of a model and the true output. It is a crucial component in various machine learning algorithms, particularly in supervised learning tasks like regression and classification. The goal of a loss function is to guide the model towards minimizing the discrepancy between its predictions and the ground truth.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23f9ae9",
   "metadata": {},
   "source": [
    "# Mean Squared Error (MSE)\n",
    "\n",
    "\n",
    "In the case of regression tasks, a common loss function is Mean Squared Error (MSE). The mathematical equation for this is:\n",
    "\n",
    "$$ L(θ) = \\frac{1}{N} \\sum_{i=1}^{N} (ŷ_i - y_i)^2 $$\n",
    "\n",
    "In this equation:\n",
    "\n",
    "- $n$ is the total number of observations.\n",
    "- $y_i$ is the actual value of the output for the i-th observation.\n",
    "- $\\hat{y}_i$ is the predicted value of the output for the i-th observation.\n",
    "- $\\sum$ denotes the summation operation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb7669c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the actual values and predicted values as NumPy arrays\n",
    "actual_values = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "predicted_values = np.array([1.2, 1.8, 2.7, 3.8, 4.9])\n",
    "\n",
    "# Calculate the squared differences between actual and predicted values\n",
    "squared_errors = (actual_values - predicted_values) ** 2\n",
    "\n",
    "# Calculate the mean of squared errors to get MSE\n",
    "mse = np.mean(squared_errors)\n",
    "\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4114f7a",
   "metadata": {},
   "source": [
    "# Cross-Entropy Loss\n",
    "\n",
    "## Binary:\n",
    "\n",
    "For classification tasks, a common loss function is Cross-Entropy Loss. The mathematical equation for binary classification is:\n",
    "\n",
    "$$\n",
    "Cross\\_Entropy\\_Loss = -\\frac{1}{n} \\sum [y_i * log(\\hat{y}_i) + (1 - y_i) * log(1 - \\hat{y}_i)]\n",
    "$$\n",
    "\n",
    "In this equation:\n",
    "\n",
    "- $n$ is the total number of observations.\n",
    "- $y_i$ is the actual value of the output for the i-th observation.\n",
    "- $\\hat{y}_i$ is the predicted value of the output for the i-th observation.\n",
    "- $\\sum$ denotes the summation operation.\n",
    "\n",
    "Cross-Entropy Loss penalizes incorrect classifications much more than ones that are correct, and it's typically used in combination with softmax for multi-class classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492861aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    return -np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)) / len(y_true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d9052e",
   "metadata": {},
   "source": [
    "### Let's implement the above loss functions using dummy data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812fd29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy data for binary classification\n",
    "y_true_classification = np.array([0, 0, 1, 1, 0])\n",
    "y_pred_classification = np.array([0.1, 0.2, 0.9, 0.8, 0.1])  # These are probabilities output by a model\n",
    "\n",
    "# Calculate Cross Entropy Loss\n",
    "cel = cross_entropy_loss(y_true_classification, y_pred_classification)\n",
    "print(f\"Cross Entropy Loss: {cel}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037a9ae1",
   "metadata": {},
   "source": [
    "## PyTorch Implementation of Loss Functions\n",
    "\n",
    "PyTorch is a widely used deep learning library which provides automatic differentiation for all operations on Tensors. It is a deep learning framework and a scientific computing package.\n",
    "\n",
    "PyTorch also includes a variety of loss functions, including Mean Squared Error (MSE) and Binary Cross Entropy (BCE), which we will use in this notebook.\n",
    "\n",
    "Firstly, we need to import the necessary modules, which include `torch` and `torch.nn.functional`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1560c5",
   "metadata": {},
   "source": [
    "### MSE implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc838c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbca984",
   "metadata": {},
   "source": [
    "Now, let's create some dummy data for a regression task. We use PyTorch Tensors to hold the data. The predicted and true outputs are both continuous, as is typical for a regression task. We can then use the `torch.nn.functional.mse_loss()` function to compute the Mean Squared Error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b1e22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy data for regression\n",
    "y_true_regression = torch.tensor([1.0, 2.5, 3.6, 4.5, 5.0])\n",
    "y_pred_regression = torch.tensor([1.2, 2.3, 3.7, 4.6, 4.9])\n",
    "\n",
    "# Calculate Mean Squared Error using PyTorch\n",
    "mse = F.mse_loss(y_pred_regression, y_true_regression)\n",
    "print(f\"Mean Squared Error: {mse.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e1e278",
   "metadata": {},
   "source": [
    "### BCE(binary cross entropy) Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a645dd",
   "metadata": {},
   "source": [
    "Similarly, we create dummy data for a binary classification task. Here, the true outputs are binary (0 or 1), and the predicted outputs are probabilities representing the likelihood of the positive class. We can then use the `torch.nn.functional.binary_cross_entropy()` function to compute the Binary Cross Entropy loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e66cc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy data for binary classification\n",
    "y_true_classification = torch.tensor([0, 0, 1, 1, 0], dtype=torch.float32)\n",
    "y_pred_classification = torch.tensor([0.1, 0.2, 0.9, 0.8, 0.1], dtype=torch.float32)  # These are probabilities output by a model\n",
    "\n",
    "# Calculate Cross Entropy Loss using PyTorch\n",
    "# PyTorch's BCELoss expects probabilities, so we don't apply the sigmoid function here\n",
    "bce = F.binary_cross_entropy(y_pred_classification, y_true_classification)\n",
    "print(f\"Binary Cross Entropy Loss: {bce.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baafb8fa",
   "metadata": {},
   "source": [
    "### Softmax Function: Conversion of Raw Outputs to Probabilities\n",
    "\n",
    "The softmax function is a widely used activation function in neural networks, particularly in multi-class classification problems. It is used to transform the raw output of a neural network into a probability distribution over multiple classes. The primary purpose of the softmax function is to convert the network's final layer outputs into probabilities, making it easier to interpret and use for classification.\n",
    "\n",
    "## The Softmax Equation\n",
    "\n",
    "The softmax function takes as input a vector of real numbers and returns a probability distribution. It does this by exponentiating each element in the input vector and then normalizing the result. Here's the equation for the softmax function:\n",
    "\n",
    "$$\n",
    "\\text{Softmax}(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^{C} e^{z_j}}\n",
    "$$\n",
    "\n",
    "In this equation:\n",
    "\n",
    "- $\\text{Softmax}(z)_i$ is the i-th element of the softmax output vector.\n",
    "- $z_i$ is the i-th element of the input vector.\n",
    "- $C$ is the total number of classes.\n",
    "\n",
    "The softmax function ensures that the output vector sums to 1, making it suitable for interpreting the values as probabilities. Each element in the output vector represents the probability of the input belonging to a particular class.\n",
    "\n",
    "## Why We Need Softmax\n",
    "\n",
    "1. **Classification**: In many machine learning and deep learning classification tasks, you need to assign an input to one of several possible classes. The softmax function provides a way to model and predict class probabilities, which is crucial for classification.\n",
    "\n",
    "2. **Interpretability**: The softmax function converts raw model outputs into probabilities, making it easier to understand the model's confidence in its predictions. This is particularly important when making decisions based on the model's output, such as in medical diagnosis or autonomous driving.\n",
    "\n",
    "3. **Training Objective**: In the training phase of a neural network, the softmax function is often used in conjunction with the cross-entropy loss. This combination encourages the model to output high probabilities for the correct class and low probabilities for incorrect classes. It's an essential part of the learning process.\n",
    "\n",
    "4. **Multiclass Problems**: The softmax function is well-suited for problems with multiple classes, where you want to distribute the probability mass among all possible classes.\n",
    "\n",
    "In summary, the softmax function is a fundamental component of neural networks, especially in multi-class classification tasks. It transforms raw network outputs into probabilities, facilitating interpretation and training while enabling models to make confident class predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc3c50c",
   "metadata": {},
   "source": [
    "### Code for softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7810f51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    # Compute the exponentials of the input vector\n",
    "    exp_x = np.exp(x)\n",
    "    \n",
    "    # Compute the sum of exponentials for normalization\n",
    "    sum_exp_x = np.sum(exp_x, axis=1, keepdims=True)\n",
    "    \n",
    "    # Calculate the softmax probabilities for each class\n",
    "    softmax_probs = exp_x / sum_exp_x\n",
    "    \n",
    "    return softmax_probs\n",
    "\n",
    "# Example usage:\n",
    "input_vector = np.array([[2.0, 1.0, 0.1],\n",
    "                         [1.0, 3.0, 0.2]])\n",
    "\n",
    "softmax_output = softmax(input_vector)\n",
    "print(\"Softmax Output:\\n\", softmax_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99905556",
   "metadata": {},
   "source": [
    "## Cross-Entropy Loss for Multi-Class Classification\n",
    "\n",
    "Cross-entropy loss, also known as log loss, is a commonly used loss function for multi-class classification problems in machine learning. It measures the dissimilarity between the predicted probabilities for each class and the actual class labels. The goal is to minimize this loss during training to improve the model's ability to correctly classify data into multiple classes.\n",
    "\n",
    "The equation for cross-entropy loss is as follows:\n",
    "\n",
    "$$\n",
    "\\text{Cross-Entropy Loss} = - \\sum_{i=1}^{C} y_i \\log(p_i)\n",
    "$$\n",
    "\n",
    "In this equation:\n",
    "\n",
    "- $y_i$ is the true label for class $i$, which is 1 for the true class and 0 for all other classes (one-hot encoding).\n",
    "- $p_i$ is the predicted probability for class $i$ as provided by the model's softmax function. The softmax function ensures that the predicted probabilities sum up to 1 across all classes.\n",
    "- $\\log(p_i)$ computes the natural logarithm of the predicted probability.\n",
    "- The summation $\\sum_{i=1}^{C}$ is taken over all classes.\n",
    "\n",
    "For a batch of data points, you typically calculate the average cross-entropy loss. The goal during training is to minimize this loss by adjusting the model's parameters (weights and biases) using gradient descent or another optimization algorithm.\n",
    "\n",
    "In summary, cross-entropy loss quantifies the dissimilarity between predicted probabilities and true class labels, making it a useful loss function for multi-class classification tasks. The objective is to minimize this loss to improve the model's classification accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc16a92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Dummy ground truth labels (one-hot encoded)\n",
    "# Suppose you have 3 classes\n",
    "y_true = np.array([[1, 0, 0],  # Class 0\n",
    "                   [0, 1, 0],  # Class 1\n",
    "                   [0, 0, 1]]) # Class 2\n",
    "\n",
    "# Dummy predicted probabilities from the softmax layer\n",
    "# These should sum to 1 for each sample\n",
    "y_pred = np.array([[0.7, 0.2, 0.1],  # Predicted probabilities for class 0\n",
    "                   [0.2, 0.5, 0.3],  # Predicted probabilities for class 1\n",
    "                   [0.1, 0.3, 0.6]]) # Predicted probabilities for class 2\n",
    "\n",
    "# Calculate the cross-entropy loss\n",
    "# Using the formula: -Σ(y_true * log(y_pred))\n",
    "cross_entropy_loss = -np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "# Print the cross-entropy loss\n",
    "print(\"Cross-Entropy Loss:\", cross_entropy_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d42760",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "In this example:\n",
    "\n",
    "- `y_true` represents the ground truth labels in one-hot encoded format for three samples and three classes.\n",
    "- `y_pred` represents the predicted probabilities from a softmax layer for the same three samples and three classes. The predicted probabilities should sum to 1 for each sample.\n",
    "- The cross-entropy loss is calculated using the formula:\n",
    "\n",
    "  $$\n",
    "  \\text{Cross-Entropy Loss} = -\\sum(y_{\\text{true}} \\cdot \\log(y_{\\text{pred}})\n",
    "  $$\n",
    "\n",
    "  This formula computes the element-wise product of the true labels $ y_{\\text{true}} $ and the natural logarithm $ \\log $ of the predicted probabilities $ y_{\\text{pred}} $. The negative sign is used to flip the sign of the sum.\n",
    "\n",
    "- The code prints the computed cross-entropy loss.\n",
    "\n",
    "Cross-entropy loss is a key metric for evaluating the performance of multi-class classification models and is often used as the objective function during model training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239c5e10",
   "metadata": {},
   "source": [
    "# Kullback-Leibler Divergence\n",
    "\n",
    "Kullback-Leibler (KL) Divergence is a measure of how one probability distribution is different from another. It's often used in machine learning algorithms when we want to compare a learned model distribution to the actual data distribution.\n",
    "\n",
    "The KL divergence of Q from P, often denoted as D(P || Q), is defined as follows:\n",
    "\n",
    "For discrete probability distributions P and Q:\n",
    "\n",
    "D(P || Q) = Σ P(x) * log(P(x) / Q(x))\n",
    "\n",
    "And for continuous distributions:\n",
    "\n",
    "D(P || Q) = ∫ P(x) * log(P(x) / Q(x)) dx\n",
    "\n",
    "In both formulas, the sum or integral is over the range of all possible outcomes, x.\n",
    "\n",
    "It's important to note that KL divergence is not symmetric, which means D(P || Q) ≠ D(Q || P).\n",
    "\n",
    "Here's how to compute KL divergence in PyTorch. Assume `p` and `q` are your two probability distributions which are PyTorch tensors.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assume p and q are your two tensors representing probability distributions.\n",
    "# These are just example tensors, replace with your actual tensors.\n",
    "p = torch.tensor([0.1, 0.2, 0.3, 0.4])\n",
    "q = torch.tensor([0.2, 0.2, 0.2, 0.4])\n",
    "\n",
    "# KL divergence\n",
    "kl_div = F.kl_div(p.log(), q, reduction='batchmean')\n",
    "print(kl_div)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416aa47e",
   "metadata": {},
   "source": [
    "## Pytorch Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bb2f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assume p and q are your two tensors representing probability distributions.\n",
    "# These are just example tensors, replace with your actual tensors.\n",
    "p = torch.tensor([0.1, 0.2, 0.3, 0.4])\n",
    "q = torch.tensor([0.2, 0.2, 0.2, 0.4])\n",
    "\n",
    "# KL divergence\n",
    "kl_div = F.kl_div(p.log(), q, reduction='batchmean')\n",
    "print(kl_div)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4c81ec",
   "metadata": {},
   "source": [
    "### In this section we will see all the topic that we have learned implemented together. In this section we will use pytorchs built in loss function and optimizer. We will also use a cifar10 dataset to train our model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03764ca5",
   "metadata": {},
   "source": [
    "Before we begin let's see how transform module work in pytorch. Because we will be using this module in order to pre-process our image. \n",
    "\n",
    "In PyTorch, the transform module provides a set of common image transformations that can be applied to datasets. These transformations allow you to preprocess and augment the data before feeding it into the neural network. The transform module is part of the torchvision library, which is a PyTorch package specifically designed for working with image datasets.\n",
    "\n",
    "The transform module provides various operations such as resizing, cropping, normalization, and data augmentation techniques like random rotations, flips, and color jittering. These transformations can be applied to both the training and test datasets to ensure consistency and appropriate preprocessing.\n",
    "\n",
    "Here's a sample code snippet that demonstrates the usage of the transform module in PyTorch:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86f2a93",
   "metadata": {},
   "source": [
    "#### NOTE: needs to separate all the code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89318b7d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the MLP class\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Set the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set the hyperparameters\n",
    "input_size = 32 * 32 * 3\n",
    "hidden_size = 200\n",
    "output_size = 10\n",
    "num_epochs = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Load and preprocess the CIFAR-10 dataset\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                             download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                            download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "# Create an instance of the MLP model\n",
    "mlp = MLP(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mlp.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "total_step = len(train_loader)\n",
    "loss_list = []\n",
    "accuracy_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Move tensors to the configured device\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = mlp(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            # Calculate training accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            accuracy = correct / labels.size(0)\n",
    "            accuracy_list.append(accuracy)\n",
    "            # Track the loss\n",
    "            loss_list.append(loss.item())\n",
    "\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{total_step}], Loss: {loss.item()}, Accuracy: {accuracy}\")\n",
    "\n",
    "# Testing\n",
    "mlp.eval()  # Switch to evaluation mode\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = mlp(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = correct / total\n",
    "    print(f\"Accuracy on the test set: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53238221",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(minibatch_cost)), minibatch_cost)\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.xlabel('Minibatch')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(len(epoch_cost)), epoch_cost)\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a7f80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(net, data_loader):\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for features, targets in data_loader:\n",
    "            features = features.view(-1, 28*28).to(DEVICE)\n",
    "            targets = targets.to(DEVICE)\n",
    "            logits = net.forward(features)\n",
    "            predicted_labels = torch.argmax(logits, 1)\n",
    "            num_examples += targets.size(0)\n",
    "            correct_pred += (predicted_labels == targets).sum()\n",
    "        return correct_pred.float()/num_examples * 100\n",
    "    \n",
    "print('Training Accuracy: %.2f' % compute_accuracy(model, train_loader))\n",
    "print('Test Accuracy: %.2f' % compute_accuracy(model, test_loader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
